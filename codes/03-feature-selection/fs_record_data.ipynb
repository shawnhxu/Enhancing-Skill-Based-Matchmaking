{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Will be utilizing feature selection on the record data dataset \"league_combined_with_chall_cleaned.csv\"\n",
    "\n",
    "Objective: to find the most relevant variables to improve Naive Bayes Classification model\n",
    "\"\"\"\n",
    "\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the record dataset\n",
    "record_data_path = '../../data/cleaned_riot_data/league_combined_with_chall_cleaned.csv'\n",
    "combined_league_ranks = pd.read_csv(record_data_path, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Should only have label with record data in the dataframe\n",
    "\"\"\"\n",
    "\n",
    "#modify the variables a bit to remove time and include it as a rate for other variables\n",
    "#somewhat a feature extraction method :)\n",
    "#do this for variables: kills, deaths, dmgObj, dmgTurr, vision_score, totalDmg, totalDmgTaken, totalMinions, gold\n",
    "combined_league_ranks['kills/min'] = combined_league_ranks['kills']/combined_league_ranks['time']\n",
    "combined_league_ranks['deaths/min'] = combined_league_ranks['deaths']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmgObj/min'] = combined_league_ranks['dmgObj']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmgTurr/min'] = combined_league_ranks['dmgTurr']/combined_league_ranks['time']\n",
    "combined_league_ranks['vision_score/min'] = combined_league_ranks['vision_score']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmg/min'] = combined_league_ranks['totalDmg']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmgTaken/min'] = combined_league_ranks['totalDmgTaken']/combined_league_ranks['time']\n",
    "combined_league_ranks['minions/min'] = combined_league_ranks['totalMinions']/combined_league_ranks['time']\n",
    "combined_league_ranks['gold/min'] = combined_league_ranks['gold']/combined_league_ranks['time']\n",
    "\n",
    "\n",
    "#remove the columns that did not include time as a rate\n",
    "combined_league_ranks = combined_league_ranks.drop('kills', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('deaths', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('dmgObj', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('dmgTurr', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('vision_score', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('totalDmg', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('totalDmgTaken', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('totalMinions', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('gold', axis=1)\n",
    "\n",
    "#also remove time column as its included in time dependent variables\n",
    "#also remove position and win column as its not record data\n",
    "combined_league_ranks = combined_league_ranks.drop('time', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('position', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('win', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before feature selecting, we want to first split the original dataset into training, validation, and testing sets with a 80%/10%/10% split.\n",
    "This is done with scikit-learn's train_test_split function.\n",
    "This is important so that the model doesn't overfit.\n",
    "\"\"\"\n",
    "\n",
    "#split combined_league_ranks first with sklearn function into training and validation/testing (80%/20%)\n",
    "# Use train_test_split function from scikit-learn\n",
    "# Use random_state = 42 for static seed\n",
    "combined_train_df, combined_vali_test_df = train_test_split(combined_league_ranks, test_size=0.2, random_state=42)\n",
    "\n",
    "#split combined_vali_test_df to split into validation and testing sets (50%/50%)\n",
    "combined_vali_df, combined_test_df = train_test_split(combined_vali_test_df, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the merit and maximize merit functions from lab 3.2\n",
    "\n",
    "def merit(x,y,correlation=\"pearson\"):\n",
    "    # x=matrix of features\n",
    "    # y=matrix (or vector) of targets\n",
    "    # correlation=\"pearson\" or \"spearman\"\n",
    "\n",
    "    # INSERT CODE HERE\n",
    "    k = x.shape[1]\n",
    "    if correlation == \"spearman\":\n",
    "      spearman_correlations = np.array([scipy.stats.spearmanr(x[:, i], y.flatten()).correlation for i in range(x.shape[1])])\n",
    "      r_xy = np.mean(spearman_correlations)\n",
    "      correlation_matrix, _ = scipy.stats.spearmanr(x, axis=0)\n",
    "      r_xx = np.mean(correlation_matrix[~np.eye(correlation_matrix.shape[0], dtype=bool)])\n",
    "      num = k * r_xy\n",
    "      denom = np.sqrt(k + k * (k - 1) * r_xx)\n",
    "      merit = num/denom\n",
    "      return merit\n",
    "    else:\n",
    "      pearson_correlations = np.array([scipy.stats.pearsonr(x[:, i], y.flatten()).correlation for i in range(x.shape[1])])\n",
    "      r_xy = np.mean(pearson_correlations)\n",
    "      pearson_correlation_matrix = np.zeros((x.shape[1], x.shape[1]))\n",
    "      for i in range(x.shape[1]):\n",
    "          for j in range(x.shape[1]):\n",
    "              corr, _ = scipy.stats.pearsonr(x[:, i], x[:, j])\n",
    "              pearson_correlation_matrix[i, j] = corr\n",
    "      r_xx = np.mean(pearson_correlation_matrix[~np.eye(pearson_correlation_matrix.shape[0], dtype=bool)])\n",
    "      num = k * r_xy\n",
    "      denom = np.sqrt(k + k * (k - 1) * r_xx)\n",
    "      merit = num/denom\n",
    "      return merit\n",
    "\n",
    "def maximize_CFS(x,y):\n",
    "     top_merit = 0\n",
    "     count = 0\n",
    "     current_subset = 0\n",
    "     # INSERT CODE HERE\n",
    "     list1 = [*range(1,x.shape[1])]; #print(list1)\n",
    "     for L in range(1,len(list1) + 1):\n",
    "          for subset in itertools.combinations(list1, L):\n",
    "               new_x = x[:, [idx - 1 for idx in subset]]\n",
    "               current_merit = merit(new_x,y,correlation=\"pearson\")\n",
    "               if current_merit > top_merit:\n",
    "                    top_merit = current_merit\n",
    "                    count += 1\n",
    "                    current_subset = list(subset)\n",
    "                    print(\"found new max merit: \", top_merit)\n",
    "                    print(\"optimal features = \", current_subset)\n",
    "                    print(\"iteration = \", count, current_subset, top_merit)\n",
    "               else:\n",
    "                    top_merit\n",
    "                    count\n",
    "     return top_merit, count, current_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After splitting the dataset into training, validation, and testing I make my target array y_test containing the ranks of each index of match data.\n",
    "\"\"\"\n",
    "\n",
    "#since there are multiple string labels used for ranks, they should be converted into numericals\n",
    "#iron = 0, bronze = 1, silver = 2, gold = 3, platinum = 4, emerald = 5, diamond = 6, challenger = 7\n",
    "y_train = []\n",
    "for i in range(0,combined_train_df.shape[0]):\n",
    "    #convert strings to int tags\n",
    "    #row i and column 0 (rank)\n",
    "    if (combined_train_df.iloc[i,0] == 'iron'):\n",
    "        y_train.append(0)\n",
    "    if (combined_train_df.iloc[i,0] == 'bronze'):\n",
    "        y_train.append(1)\n",
    "    if (combined_train_df.iloc[i,0] == 'silver'):\n",
    "        y_train.append(2)\n",
    "    if (combined_train_df.iloc[i,0] == 'gold'):\n",
    "        y_train.append(3)\n",
    "    if (combined_train_df.iloc[i,0] == 'platinum'):\n",
    "        y_train.append(4)\n",
    "    if (combined_train_df.iloc[i,0] == 'emerald'):\n",
    "        y_train.append(5)\n",
    "    if (combined_train_df.iloc[i,0] == 'diamond'):\n",
    "        y_train.append(6)\n",
    "    if (combined_train_df.iloc[i,0] == 'challenger'):\n",
    "        y_train.append(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1811654963201674\n",
      "0.20199468597917908\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now we want to subset the training dataset and typecast them into arrays to input into custom function called merit to test it out.\n",
    "\"\"\"\n",
    "\n",
    "#subset combined_league_ranks to include everything but the label column \"rank\"\n",
    "combined_train_features = combined_train_df.iloc[:, 1:]\n",
    "#typecast new features subset into array for merit function\n",
    "x_train = combined_train_features.values\n",
    "#also typecast target list into array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#test merit function on x_record and y_record to find merit of using all features\n",
    "#compare \"spearman\" and \"pearson\" correlations\n",
    "print(merit(x_train, y_train, correlation='spearman'))\n",
    "print(merit(x_train, y_train, correlation='pearson'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xusha\\anaconda3\\envs\\dsan5000\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\xusha\\anaconda3\\envs\\dsan5000\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found new max merit:  0.01497226167743923\n",
      "optimal features =  [1, 2]\n",
      "iteration =  1 [1, 2] 0.01497226167743923\n",
      "found new max merit:  0.08760598416913734\n",
      "optimal features =  [1, 3]\n",
      "iteration =  2 [1, 3] 0.08760598416913734\n",
      "found new max merit:  0.15295848815963234\n",
      "optimal features =  [1, 5]\n",
      "iteration =  3 [1, 5] 0.15295848815963234\n",
      "found new max merit:  0.1804032826105836\n",
      "optimal features =  [3, 5]\n",
      "iteration =  4 [3, 5] 0.1804032826105836\n",
      "found new max merit:  0.21731546518230835\n",
      "optimal features =  [3, 8]\n",
      "iteration =  5 [3, 8] 0.21731546518230835\n",
      "found new max merit:  0.25191565673706656\n",
      "optimal features =  [5, 6]\n",
      "iteration =  6 [5, 6] 0.25191565673706656\n",
      "found new max merit:  0.30437172388655104\n",
      "optimal features =  [5, 8]\n",
      "iteration =  7 [5, 8] 0.30437172388655104\n",
      "found new max merit:  0.33380613045134044\n",
      "optimal features =  [3, 5, 8]\n",
      "iteration =  8 [3, 5, 8] 0.33380613045134044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.33380613045134044, 8, [3, 5, 8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since merit function seems to work, I now will run through the training set features to see which subset of features maximizes merit. \n",
    "This will feature select which training set features are most important in correlating with the target array (ranks)\n",
    "\"\"\"\n",
    "\n",
    "#use the maximize CFS to iterrate through all combinations of features then outputs subset of features that maximize merit\n",
    "maximize_CFS(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion part 1\n",
    "From the \"maximize_CSF\" function to see which features maximizes the performance metrics, we see that the subset of features numbers [3,5,6] outputs the highest merit. Thus after correlation-based feature selection we can conclude that features 3, 5, and 8 (Objective Damage per minute, vision score per minute, and minions farmed per minute) give the highest merit and thus should be used for Naive Bayes classification.<br>\n",
    "I also want to test what features are outputted if I were to remove the positions \"jungle\" and \"utility\" as they have the more odd values compared to the other positions (top, middle, and bottom lanes) that show more consistent values with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xusha\\anaconda3\\envs\\dsan5000\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\xusha\\anaconda3\\envs\\dsan5000\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found new max merit:  0.018211033515251038\n",
      "optimal features =  [1, 2]\n",
      "iteration =  1 [1, 2] 0.018211033515251038\n",
      "found new max merit:  0.04726896631776993\n",
      "optimal features =  [1, 3]\n",
      "iteration =  2 [1, 3] 0.04726896631776993\n",
      "found new max merit:  0.06600750656808484\n",
      "optimal features =  [1, 4]\n",
      "iteration =  3 [1, 4] 0.06600750656808484\n",
      "found new max merit:  0.10733537084682764\n",
      "optimal features =  [1, 5]\n",
      "iteration =  4 [1, 5] 0.10733537084682764\n",
      "found new max merit:  0.2983176267940218\n",
      "optimal features =  [1, 8]\n",
      "iteration =  5 [1, 8] 0.2983176267940218\n",
      "found new max merit:  0.3445614331024441\n",
      "optimal features =  [2, 8]\n",
      "iteration =  6 [2, 8] 0.3445614331024441\n",
      "found new max merit:  0.36711165733632656\n",
      "optimal features =  [5, 8]\n",
      "iteration =  7 [5, 8] 0.36711165733632656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.36711165733632656, 7, [5, 8])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat everything but this time after removing jungle and utility positions\n",
    "#re-import data to redo the process\n",
    "combined_league_ranks = pd.read_csv(record_data_path, index_col=None)\n",
    "\n",
    "#re-clean\n",
    "combined_league_ranks['kills/min'] = combined_league_ranks['kills']/combined_league_ranks['time']\n",
    "combined_league_ranks['deaths/min'] = combined_league_ranks['deaths']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmgObj/min'] = combined_league_ranks['dmgObj']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmgTurr/min'] = combined_league_ranks['dmgTurr']/combined_league_ranks['time']\n",
    "combined_league_ranks['vision_score/min'] = combined_league_ranks['vision_score']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmg/min'] = combined_league_ranks['totalDmg']/combined_league_ranks['time']\n",
    "combined_league_ranks['dmgTaken/min'] = combined_league_ranks['totalDmgTaken']/combined_league_ranks['time']\n",
    "combined_league_ranks['minions/min'] = combined_league_ranks['totalMinions']/combined_league_ranks['time']\n",
    "combined_league_ranks['gold/min'] = combined_league_ranks['gold']/combined_league_ranks['time']\n",
    "\n",
    "combined_league_ranks = combined_league_ranks.drop('kills', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('deaths', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('dmgObj', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('dmgTurr', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('vision_score', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('totalDmg', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('totalDmgTaken', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('totalMinions', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('gold', axis=1)\n",
    "\n",
    "combined_league_ranks = combined_league_ranks.drop('time', axis=1)\n",
    "combined_league_ranks = combined_league_ranks.drop('win', axis=1)\n",
    "\n",
    "\n",
    "#remove jungle and utility positions\n",
    "combined_league_ranks = combined_league_ranks[combined_league_ranks['position'] != 'jungle']\n",
    "combined_league_ranks = combined_league_ranks[combined_league_ranks['position'] != 'utility']\n",
    "#then remove position column\n",
    "combined_league_ranks = combined_league_ranks.drop('position', axis=1)\n",
    "\n",
    "\n",
    "#split process\n",
    "combined_train_df, combined_vali_test_df = train_test_split(combined_league_ranks, test_size=0.2, random_state=42)\n",
    "#split again for vali and testing\n",
    "combined_vali_df, combined_test_df = train_test_split(combined_vali_test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "#remake y_train\n",
    "#iron = 0, bronze = 1, silver = 2, gold = 3, platinum = 4, emerald = 5, diamond = 6, challenger = 7\n",
    "y_train = []\n",
    "for i in range(0,combined_train_df.shape[0]):\n",
    "    #convert strings to int tags\n",
    "    #row i and column 0 (rank)\n",
    "    if (combined_train_df.iloc[i,0] == 'iron'):\n",
    "        y_train.append(0)\n",
    "    if (combined_train_df.iloc[i,0] == 'bronze'):\n",
    "        y_train.append(1)\n",
    "    if (combined_train_df.iloc[i,0] == 'silver'):\n",
    "        y_train.append(2)\n",
    "    if (combined_train_df.iloc[i,0] == 'gold'):\n",
    "        y_train.append(3)\n",
    "    if (combined_train_df.iloc[i,0] == 'platinum'):\n",
    "        y_train.append(4)\n",
    "    if (combined_train_df.iloc[i,0] == 'emerald'):\n",
    "        y_train.append(5)\n",
    "    if (combined_train_df.iloc[i,0] == 'diamond'):\n",
    "        y_train.append(6)\n",
    "    if (combined_train_df.iloc[i,0] == 'challenger'):\n",
    "        y_train.append(7)\n",
    "\n",
    "\n",
    "#typecast again and get x_train\n",
    "#subset combined_league_ranks to include everything but the label column \"rank\"\n",
    "combined_train_features = combined_train_df.iloc[:, 1:]\n",
    "#typecast new features subset into array for merit function\n",
    "x_train = combined_train_features.values\n",
    "#also typecast target list into array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "#finally feature select again with maximize_CFS function\n",
    "maximize_CFS(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion part 2\n",
    "We see that still features 5 and 8 (vision score per minute and minions farmed per minute) are the most defining features in maximizing merit after removing \"jungle\" and \"utility\" positions. So I will continue to use those features in Naive Bayes. As for feature 3 (objective damage per minute), I will include it still for Naive Bayes to first examine how well it works in the model when including the \"jungle\" position."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
