{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification Introduction\n",
    "<br>\n",
    "__Objective__\n",
    "To develop a model that can predict unseen data to some classification is the primary purpose of Naive Bayes Classification. So to reach this objective of making a model that can accurately predict some class given sample data, this process would then employ multiple statistical concepts.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Foundational Knowledge__<br>\n",
    "Naive Bayes requires the use of Bayes' Theorem which is a probability concept that illustrates the probability of some situation, hypothesis, or case occurring based on prior understandings (prior probabilities). Thus, Naive Bayes is grounded in training probabilities from frequencies of features for the purpose of classification predictions. <br>\n",
    "In Naive Bayes, for it to be \"naive\" means that the features/variables being used for the classification prediction are independent of other variables.\n",
    "\n",
    "![Bayes Theorem Image [Source](https://seeve.medium.com/machine-learning-bayes-theorem-2f48c33d51e5)](images/bayes_theory.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "__Steps__<br>\n",
    "The concept of Naive Bayes employs prior probabilities, the likelihood of data collected per class label, output a posterior probability for each class, and finally assign the input with a class label with the highest posterior probability.\n",
    "<br><br>\n",
    "\n",
    "__The Aim of Naive Bayes in this Project__<br>\n",
    "As mentioned before, in order to find ways of enhancing competitive video games' skill-based matchmaking systems there needs to be analysis of how certain variables within a game determines one rank. So to do this, I aim to use Naive Bayes to create a prediction model that will output a prediction for data inputs. By using the collected values of variables from ranked players and their associated ranks as class labels, I have everything I need to utilize Naive Bayes in building a classification algorithm that will predict some unseen player data's rank. <br>\n",
    "This will require me to first train my model with my dataset and then later be tested with unseen data.\n",
    "<br><br>\n",
    "\n",
    "![Naive Bayes Image [Source](https://medium.com/@dancerworld60/demystifying-na%C3%AFve-bayes-simple-yet-powerful-for-text-classification-ad92b14a5c7)](images/naive_bayes.png)\n",
    "\n",
    "__Different Variants of Naive Bayes__<br>\n",
    "\n",
    "- __Gaussian Naive Bayes:__ This is when the features are continuous and the values follow a Gaussian Distribution. Since it assumes features for each class are Gaussian distributed, then within each class they would find the mean and standard deviation for each feature for classification.\n",
    "  \n",
    "- __Multinomial Naive Bayes:__ This time the features are assumed to follow a multinomial distribution. This type of Naive Bayes is pretty common with text data that utilize word counts for text classification.\n",
    "  \n",
    "- __Bernoulli Naive Bayes:__ Since Bernoulli is about some outcome being one or the other, this type of Naive Bayes assumes features are binary meaning the values could be one of two options: Present or Absent, 1 or 0, True or False.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "An important step in preparing data for Naive Bayes Classification is to split the dataset into __training__, __validation__, and __testing__ subsets. This is important because when building the model for predictions, we want the model to avoid overfitting onto the training data. This means that the model only correctly performs with the training dataset. To make a good classification model, one would look to make a model that performs well in general. The splitting will be __80% training, 10% validation, and 10% testing__.\n",
    "\n",
    "![Data Splitting Image [Source](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk/learning-resources/training-testing-machine-learning-models/training-testing-evaluating-machine-learning-models)](images/split_data.png)\n",
    "<br><br>\n",
    "The training data subset would be used to teach the model the patterns of the features being used for classification. I like to think the model learns the general benchmark or description of each class which can be used to compare with future unseen data to output a prediction.\n",
    "<br><br>\n",
    "Sometimes the validation part of the model building process isn't included but it is an important step. This step utilizes the learned model from the training set to further tune the hyperparameters of the model. This step is to prevent overfitting of the data to the training data subset. Basically, this step is to test the model so far and to adjust anything if needed.\n",
    "<br><br>\n",
    "Now with the testing set, it is the part where the final evaluation of the model's performance on unseen data. This is to test how general the model and how effective it is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: https://www.grandviewresearch.com/industry-analysis/video-game-market#:~:text=b.-,The%20global%20video%20game%20market%20size%20was%20estimated%20at%20USD,USD%20583.69%20billion%20by%202030.\n",
    "[^2]: https://www.microsoft.com/en-us/research/wp-content/uploads/2006/10/Game-Developer-Feature-Article-Graepel-Herbrich.pdf\n",
    "[^3]: https://cs.stanford.edu/people/paulliu/files/www-2021-elor.pdf\n",
    "[^4]: https://ieeexplore.ieee.org/abstract/document/4938634\n",
    "[^5]: https://ieeexplore.ieee.org/abstract/document/9231859\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsection: Feature Selection for Record Data\n",
    "\n",
    "Link to Source Record Data: [Record Data Source](https://github.com/shawnhxu/EnhancingSBMM/tree/main/data/01-raw-riot-data/league-data) <br><br>\n",
    "\n",
    "Link to source of feature selecting code on Github: [Feature Select Record Data Code](https://github.com/shawnhxu/EnhancingSBMM/blob/main/codes/03-feature-selection/fs_record_data.ipynb) <br><br>\n",
    "\n",
    "In this section I look into identifying the most important and relevant features of the dataset in the training set. This was done with pearson correlation, merit performance metric, and iterrating through all combination of features in the training data set also known as Correlation-based Feature Selection (CFS). <br><br>\n",
    "\n",
    "After using CFS on my record data, I was given the output of feature 3, 5, and 8 which were the Damage to Objectives per minute, Vision score per minute, and Minions farmed per minute features that maximized my performance metric (merit). Thus from this we can confirm that these features would be the features that should be used in my Naive Bayes classification model to improve the model accuracy and computation time. <br><br>\n",
    "\n",
    "After performing this on the dataset, I was curious as to see which subset of features would be outputted after removing the roles \"jungle\" and \"utility\" as they showed the most outlier-like values during EDA. And after going through the feature selection process again, the features 5 and 8 were outputted as the best subset of features to maximize merit. Thus we can confirm that vision score per minute and minions farmed are the best features in predicting rank as these features overlap with the features outputted previously before removing jungle and utility positions. <br><br>\n",
    "\n",
    "Moving forward, I will still include column 3 and positions jungle and utility in the model to test the overall effectiveness of rank predicting for the general player population which includes all positions played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsection: Feature Selection for Text Data\n",
    "\n",
    "Link to Text Data on Github: [Source of Text Data](https://github.com/shawnhxu/EnhancingSBMM/tree/main/data/01-raw-riot-data/league-text-data) \n",
    "<br><br>\n",
    "\n",
    "My idea for Naive Bayes for text data was utilize the champions played by players in each match to train a model to predict which position each champion typically plays. Although, this is less about predicting and more so about determining which position each champion typically plays according to retrieved match data. \n",
    "<br><br>\n",
    "\n",
    "Since the features in this case are the Champions played, then the use of feature selection for my text data is not needed at all as each feature is unique/independent and is significant in determining the feature's classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Record Data Results\n",
    "\n",
    "Link to Naive Bayes Code for Record Data on Github: [NB Code for Record Data](https://github.com/shawnhxu/EnhancingSBMM/blob/main/codes/04-naive-bayes/nb_record_data.ipynb) \n",
    "<br><br>\n",
    "\n",
    "*Note Numerics for Ranks: 0 = Iron, 1 = Bronze, 2 = Silver, 3 = Gold, 4 = Platinum, 5 = Emerald, 6 = Diamond, 7 = Challenger*<br>\n",
    "\n",
    "For my record data, since the values are continuous I decided on using Scikit-Learn's function called GaussianNB() which allowed me to fit my model to the the training dataset. I then used this model to create predictions for unseen data in the Validation dataset as well as the Testing dataset.<br>\n",
    "This is what I found: <br><br>\n",
    "\n",
    "__Accuracies:__<br>\n",
    "I compared the accuracy of the predicition model to each split dataset (training, validation, and testing) to compare how overfit the model (or not overfit). And here are the accuracy results:<br>\n",
    "\n",
    "![Accuracies Image](images/accuracies_record.png)\n",
    "\n",
    "From these results we can see that the accuracies are actually quite similar with each other for training, validation, and testing datasets. So, thankfully there doesn't seem to be any overfitting. That being said, the overall accuracy does seem to be low with about 20% accuracy in correctly predicting the rank of unseem match data. But, since the model is generally performing the same across all subsets of data, then I confidently conclude that overfitting is not occurring but not for underfitting. Since my accuracy is low, I suspect that my model is not learning patterns from the retrieved data. This was somewhat what I was expecting as there are too many factors in a competitive Ranked Match for League of Legends and that I either did not get enough variables from my data retrieval or that it simply an impossible task to do accurately. <br><br>\n",
    "\n",
    "__Recall and Precision:__<br>\n",
    "Next, I looked into recall and precision of each class or Rank from the predicition model and this is what I found:<br>\n",
    "\n",
    "![Precision and Recall Image](images/preci_recall_record.png)\n",
    "\n",
    "Precision describes what the ratio of all the predicted positives, which were actually positive. From the plot shown above, we can see that the precision scores are generally pretty low across the board except for ranks: Bronze, Gold, and Challenger. Bronze and Challenger ranks are on the opposite sides of the spectrum so this could be because the model was able to learn what lower-ranks and what top players look like. However, I do not really understand why the rank Gold has such a high precision score as they are part of the middle pack in terms of rank.<br><br>\n",
    "\n",
    "Recall on the other hand seems to be more consistent, though consistently low. Recall describes the ratio of the number predicted as true among the actually true. So the model in this case does not really get the actually true cases too often with the highest recall being a ratio of about 0.35. <br><br>\n",
    "\n",
    "Combining recall and precision, you get the F1-Score which explains how well the model performs. In this case, it outputs about 0.19 which follows the model accuracy result of about 20% accuracy.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "__Confusion Matrix:__<br>\n",
    "Now for the confusion matrix to illustrate how the trained model performed on the testing dataset:<br>\n",
    "\n",
    "![Confusion Matrix Image](images/conf_matrix_record.png)\n",
    "\n",
    "After showing the results and performance metrics, the confusion matrix does seem to illustrate how poorly it was able to correctly predict classes.<br><br>\n",
    "Something that I noticed in this plot is that the 2nd rank or Silver seems to have no predictions at all while the 3rd rank or Gold has the majority of predictions. This was also highlighted earlier in the precision score. This investigation is leading me to believe that the ranks Silver and Gold have some anomalies in the data.\n",
    "\n",
    "__Conclusions:__<br>\n",
    "I think if I were to do this again, I would possibly look into retrieving more unique features for my dataset. After observing the performance of the training model on the Validation and Testing set, I realized that the data might be way too vague for the model to learn the patterns needed to accurately predict some unseen player's rank based solely on their match data. <br>\n",
    "\n",
    "Though this adds to the difficulty of creating a well-working Skill-Based Matchmaking System for competitive video games like *League of Legends* as there are simply too many factors that can influence the outcome of a game. That being said, I did find that feature selecting did slightly work in making a more accurate model. From my Naive Bayes code, I made a model that had about a 20% accuracy in predicting rank but without feature selecting there was a slight decrease in accuracy (about 2% decrease in accuracy). This meant that there are indeed variables within a game that should be valued when determining a player's skill level. There are still, however, many more variables that I could look into if I were to continue this project so there are possible features in match data that might benefit this rank prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Text Data Results:\n",
    "\n",
    "Link to Naive Bayes Code for Text Data on Github: [NB Code for Text Data](https://github.com/shawnhxu/EnhancingSBMM/blob/main/codes/04-naive-bayes/nb_text_data.ipynb)\n",
    "\n",
    "Doing somewhat similar task but with text classification this time, I performed a similar process as the record data. The only difference is that I used a vectorizer from Scikit-Learn to process the string data as numerical data before fitting the text data into a NB Model. Also, I used MultinomialNB() from Scikit-Learn as well to fit the text data into classes.<br><br>\n",
    "\n",
    "What I did in this section was basically retrieve a dataset of the champions played in a match and the role the champion played by the player. I would then train a model to predict unseen data on where certain champions might play in the unseen match data. I then calculated metrics for how well this model worked on the retrieved text data with associated labels. <br><br>\n",
    "\n",
    "__Accuracies:__<br>\n",
    "\n",
    "Since this model was straightforward and performed as well as I expected I did not include a validation subset this time for the sake of time. Here is what I found in regards to the accuracy of the NB model:<br>\n",
    "\n",
    "![Accuracies Image](images/accuracies_text.png)\n",
    "\n",
    "As you can see the accuracy is about 0.86 for both training and testing datasets. This means that the model was able to recognize which characters (champions) played in what roles typically. Nothing else to say as this model worked pretty effectively. I will say that the accuracy is not perfect because in this game the players have no restrictions for what positions their characters plays which leads to unique team compositions sometimes. Though through social/community consensus, some characters are best fit in certain roles. This consensus is thus what we see in this model and in the plot shown.<br><br>\n",
    "\n",
    "Since the accuracy is high in both training and testing, I can confidently say that the model does not seem to overfit. Additionally, since the accuracy is high that there does not seem to be underfitting from the model.\n",
    "<br><br>\n",
    "\n",
    "__Precision, Recall, and F1:__<br>\n",
    "\n",
    "With such high accuracies, I wanted to confirm it with recall, precision, and finally F1-Score:<br>\n",
    "\n",
    "![Precision, Recall, and F1-Score Image](images/f1_prec_recall_text.png)\n",
    "\n",
    "We can thus confirm that the model does indeed show promising performance metrics based on the overall precision, recall, and F1-Score. All three metrics hover around 0.86 score similar to accuracy.\n",
    "\n",
    "__Confusion Matrix:__\n",
    "\n",
    "With such promising metrics, I am expecting to see a confusion matrix that has a dense diagonal line:<br>\n",
    "\n",
    "![Text Data Confusion Matrix](images/conf_matrix_text.png)\n",
    "\n",
    "As hoped, the matrix seems to fully illustrate why the metrics are so high. The model was able to correctly predict the true classes for unseen data with little incorrect predictions. Positions Jungle and Mid seem to have some overlaps in incorrect predictions which might be because the champions playing in the Mid roles might also have some play in the Jungle role or vice versa.<br><br>\n",
    "\n",
    "__Conclusions:__<br>\n",
    "\n",
    "The model has outputted promising metrics showing that the text data was able to be properly classified with the given training data. It tells us that the consensus of where champions play in a ranked match for *League of Legends* is strongly expressed by the players as the model was able to predict where each champion is typically played despite having no restrictions of what position they can play. Overall, the NB model performed very well and it can be seen in the confusion matrix with the dense diagonal line shown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
