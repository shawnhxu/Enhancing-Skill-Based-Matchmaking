{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "format:\n",
        "  html:\n",
        "    embed-resources: true\n",
        "    code-fold: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction\n",
        "\n",
        "For this clustering section, I plan to use different clustering techniques for analysis of my record data that I retrieved from the Riot API. The data contains match data collected from a multitude of players from different ranks and the cleaned version of the data can be found [here](https://github.com/shawnhxu/EnhancingSBMM/tree/main/data/02-cleaned_riot_data). <br> <br>\n",
        "\n",
        "My goal with clustering for my record data is to see if I can perform unsupervised clustering on data after removing the labels and then comparing the results to the original cluster distribution WITH labels. I want to see if my data is too confusing with the retrieved variables or is distinct enough per class label (rank). This might provide more insight on my results from Naive Bayes Classification. <br> <br>\n",
        "\n",
        "K-Means Clustering code: [link](https://github.com/shawnhxu/EnhancingSBMM/blob/main/codes/06-clustering/kmean.ipynb)\n",
        "<br>\n",
        "DBSCAN Clustering code: [link](https://github.com/shawnhxu/EnhancingSBMM/blob/main/codes/06-clustering/dbscan.ipynb)\n",
        "<br>\n",
        "Hierarchical Clustering code: [link](https://github.com/shawnhxu/EnhancingSBMM/blob/main/codes/06-clustering/hier_cluster.ipynb)\n",
        "<br> <br>\n",
        "\n",
        "### Theory for K-Means, DBSCAN, and Hierarchical Clustering\n",
        "\n",
        "Before showing my process and results, I want to go over the theory behind the three clustering techniques I will be using for my project. <br> <br>\n",
        "\n",
        "__K-Means Clustering:__ <br>\n",
        "K-means is a relatively straight-forward technique. The idea is to split the data (that does not contain any class labels) into \"K\" number of clusters. Though, this type of clustering might be relatively unreliable[^2] as the process is initialized through random cluster centers. So at the start, there will be randomly assigned centroids as mentioned before and the nearest data points are assigned to each centroid temporarily. The next steps are then iterated multiple times. <br><br>\n",
        "\n",
        "The next step is to take a new mean of the assigned data points to get new centroids. The new centroids then get assigned new points that are closest to those new centroids. This process is iterated multiple times until the centroids basically don't change anymore. This is why this technique can be unreliable as the intial centroids for each cluster are what determines the outcome of the converging centroids. This can thus be unreliable for data that has no distinct separation of data points.<br> <br>\n",
        "\n",
        "__DBSCAN Clustering:__ <br>\n",
        "Next, the Density-Based Spatial Clustering of Applications with Noise or DBSCAN technique is another unsupervised clustering technique that can be helpful in identifying outliers/noise within a dataset. This technique is similar to K-Means in that it can also assign data point to some relative point. In this case, there are assigned core points within the dataset where these points are basically assigned based on some minimum number of neighboring points. Using a specified number of points associated to each core point is thus why it is called \"density-based\". Additionally, we can use multiple distance metrics[^1] to determine which points are considered to be core points.\n",
        "<br> <br>\n",
        "This can be important for complex datasets because this technique can find points that are considered outliers or noise. That is because data points that are not close enough to any core points or are not part of some cluster with those core points might be outliers. This can thus be useful for datasets that might have many features and observations that contain noise[^5].\n",
        "<br><br>\n",
        "\n",
        "__Hierarchical Clustering:__ <br>\n",
        "There are two main methods of Hierarchical Clustering: \"Bottom-up\" or Agglomerative Hierarchical Clustering and \"Top-down\" or Divisive Hierarchical Clustering[^3]. In both cases, there is a hierarchy of clustering occurring starting from either multiple clusters combining into one (bottom-up) or one single cluster splitting into more (top-down). \n",
        "<br><br>\n",
        "For Agglomerative Hierarchical Clustering, each data point is their own cluster at first. Then depending on the strategy used, then the closest data points of two clusters, or farthest points of two clusters, or the least average distance between all data points in the two clusters merge to become one cluster. Whatever linkage strategy[^6] (i.e single, average, complete, ward linkage, etc.) is used until only one cluster remains. \n",
        "<br><br>\n",
        "For Divisive Hierarchical Clustering, it starts with the dataset as one single large cluster. This clustering then divides the data points by identifying the most dissimilar object and assigning the most similar data points to it. The remainder becoming its own cluster and thus dividing the original one cluster. \n",
        "<br><br>\n",
        "Both types of Hierarchical Clustering can be important for many reasons. One is to see the internal relationships within the dataset and within each cluster within clusters. This type of clustering can also be important for a complex dataset with unrecognizable clustering[^4]. Dendrogram visualizing also can shows how similar or dissimilar the associated clusters are. A taller vertical line in the hierarchy means the clusters are more dissimlar.\n",
        "\n",
        "[^1]: https://ieeexplore.ieee.org/abstract/document/9038535?casa_token=gA0AHEO7pRsAAAAA:cE0uJNn7uZihBu-cEejdvFOf_-uxq1EUzDZA94x8C-EduMK-rBrez8rwa0NzuO9ozJmKh8tBcQ\n",
        "\n",
        "[^2]: https://optimaldesign.com/Download/ArrayMiner/AMatMETMBS2001.pdf\n",
        "\n",
        "[^3]: https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.53?casa_token=iHd4jYEMyx0AAAAA%3Aa5hGgRtHnp0Cqbhh9b6T5pDTF4153N4PcqHT2eyA2A55Jokb6Jfn7GZbNiV73RMFx9HLrTw0BapXjik\n",
        "\n",
        "[^4]: https://onlinelibrary.wiley.com/doi/full/10.1111/biom.12647?casa_token=wk0Kvbx__LYAAAAA%3AcIj7DX7EvzLPRrEEfNpOaEwEN8T2VV9uAv3oLjN2f5xzyqlJ1kzyrfsFTIAhCzUPcEANBo5NznkhM9Y\n",
        "\n",
        "[^5]: https://ieeexplore.ieee.org/abstract/document/6814687?casa_token=1IBA78BBBEwAAAAA:9G4vbIXhVSFPhzolMwHIhmpkVq4PKKLahjh4zyhglDg33TMQ1SSaJ4OM5ctx0Lyt8MjNMvOkeQ\n",
        "\n",
        "[^6]: https://link.springer.com/chapter/10.1007/978-3-319-21903-5_8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results for K-Means Clustering\n",
        "\n",
        "For the purpose of looking into the three different clustering, it was only possible to visualize the results with low dimensions of the dataset. From the feature selection section of my project, I concluded that the most impactful features to influence a classification model would be features 5 and 8 which were the ```visions score per minute``` and ```minions farmed per minute features```. <br> <br>\n",
        "\n",
        "With my knowledge of the feature selection portion, there was one other characteristic of K-Means that needed to be understood for my dataset. As mentioned before, the \"K\" number of clusters to be used for the K-Means clustering must be initialized. The issue is finding out what number of clusters works best for my subsetted dataset. Utilizing the Scikit-Learn's K-Means cluster offers a default value of 5 clusters to be initialized but this might not be necessary for lower dimensional datasets and so I set out on using a different metric to decide.\n",
        "<br> <br>\n",
        "\n",
        "What I decided on using is the inertia of the clustering which is conveniently offered within ```Scikit-Learn```'s K-Means function when fitting my dataset with the K-Means algorithm. Within each of the calculated clusters, the inertia is calculated to describe how far apart the points are from the centroid of each of their respective cluster. So, what I am looking for is some optimal number of clusters to initialize that would minimize inertia without using too many clusters:\n",
        "<br> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#how many clusters will be initialized for k-means clustering\n",
        "clusters = [2,4,6,8,10,12]\n",
        "inertias = []\n",
        "\n",
        "#run through each number of clusters\n",
        "for num in clusters:\n",
        "    \n",
        "    #define each KMeans clustering\n",
        "    km = KMeans(n_clusters=num, random_state=33, n_init='auto') #specify certain seed for reproduceability\n",
        "    \n",
        "    #fit  current K-means with dataset\n",
        "    current_km = km.fit(subset_combined)\n",
        "\n",
        "    #save the inertia in empty list for plotting\n",
        "    inertias.append(current_km.inertia_)\n",
        "\n",
        "plt.plot(clusters, inertias)\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Inertia of Each Number of Clusters with K-Means')\n",
        "\n",
        "#save as .png\n",
        "plt.savefig('kmean_inertia.png', bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "![Optimal KMeans Clusters by Inertia](images/kmean_inertia.png)\n",
        "\n",
        "From the inertia plot, I used the Elbow Method to find where the decreasing inertia rate is diminishing. This can be seen at about 4 or even 6 clusters. For my purposes, I went with 4 clusters as I am only using a 2-dimensional subset of data. <br> <br>\n",
        "\n",
        "After subsetting the dataset and determining that 4 clusters should be used, it was time to fit it into K-Means Clustering to view how well it performed in forming clusters for the two features I was interested in. <br> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#apply optimal param n_clusters = 4\n",
        "km_3 = KMeans(n_clusters=4, random_state=33, n_init='auto')\n",
        "#now fit and predict clusters\n",
        "km_3_predict = km_3.fit_predict(subset_combined)\n",
        "#not scatterplot of the two features\n",
        "plt.scatter(subset_combined['vision_score/min'], subset_combined['minions/min'], c=km_3_predict)\n",
        "plt.scatter(km_3.cluster_centers_[:, 0], km_3.cluster_centers_[:, 1], c='red')\n",
        "#labels\n",
        "plt.title('K-Means Clustering with 4 Clusters of Features 5 and 8')\n",
        "plt.xlabel('Vision Score per Minute')\n",
        "plt.ylabel('Minions Farmed per Minute')\n",
        "\n",
        "#save as .png\n",
        "plt.savefig('kmean_cluster.png', bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "![KMeans On Subset Data](images/kmean_cluster.png)\n",
        "\n",
        "From the distribution of data, it was pretty clear how the final clusters were formed with the use of a red dot for the centroids. The data's clustering seemed pretty decent but considering how distributed the data points were in the first place, seeing the 4 clusters didn't really make as much sense. In fact, the use of a two-dimensional subset logically should give me two clusters to work with which is sort of what I see. In any case, the use of K-Means doesn't seem to work too well here.\n",
        "<br> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results for DBSCAN Clustering\n",
        "\n",
        "Now using Density-Based Spatial Clustering of Applications with Noise or DBSCAN for short, I used the same subset of data as before but this time I was interested in a different parameter to optimize for better clustering. <br> <br>\n",
        "\n",
        "In this technique, the parameter tuning I wanted to look out for was determining what the minimum density of neighboring points should be for a data point to be considered a core point (or a centroid for a cluster). To do this, I basically looked at maximizing the Silhouette score of forming clusters with different density parameters. The Silhouette score basically describes how well clustered the clusters are or how well each data point within each cluster fit in their  cluster. A high score (1) would indicate well matching with their own respective cluster labels. A low score (-1) would indicate poor matching. Here is what I found: <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#use different density parameters for DBSCAN\n",
        "density = [3,6,9,12,15]\n",
        "#will fill this list with silhouette scores of each density\n",
        "sil = []\n",
        "\n",
        "for num in density:\n",
        "    #define DBSCAN model and fit/predict the clusters\n",
        "    current_dbscan = DBSCAN(min_samples=num)\n",
        "    db_predict = current_dbscan.fit_predict(subset_combined)\n",
        "\n",
        "    #calculate silhouette score based on cluster predictions\n",
        "    sil.append(silhouette_score(subset_combined, db_predict))\n",
        "\n",
        "#plot the silhouette score per minimum num of neighboring points per cluster\n",
        "plt.plot(density, sil)\n",
        "plt.xlabel('Min # of Neighboring Points per Cluster')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score per Min # of Neighboring Points')\n",
        "\n",
        "#save as .png\n",
        "plt.savefig('dbscan_sil.png', bbox_inches='tight')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "![Silhouette Score with Different Densities](images/dbscan_sil.png)\n",
        "\n",
        "From the plot, we can see that the number of neighboring points to determine core points that maximized the Silhouette score was around 9 neighboring points. So, for fitting my subset data I will use 9 to optimize clustering for DBSCAN. <br> <br>\n",
        "\n",
        "Another thing I wanted to compare after determining the optimal density parameter, was determining which distance metric would work best in determining clusters. Distance metrics calculates the distance between data points. This is important as we are trying to determine which data points (and also how many neighbors as mentioned before) are considered closest to some data point. To see how different distance metrics worked I decided to plot 4 different distance metric methods using 9 neighboring points to DBSCAN cluster my dataset. This is what I found using ```\"Euclidean\"```, ```\"Cosine\"```, ```\"Hamming\"```, and ```\"Manhattan\"``` distance metrics: <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#use 9 minimum neighboring points\n",
        "#i used 0.25 eps because it seemed to show outliers better\n",
        "#also try different distance metrics: euclidean, cosine, hamming, and manhattan\n",
        "metrics = ['euclidean', 'cosine', 'hamming', 'manhattan']\n",
        "count = 0\n",
        "for metric in metrics:\n",
        "    #count of subplot\n",
        "    count += 1\n",
        "    #set DBSCAN with parameters\n",
        "    dbscan = DBSCAN(eps=0.25, min_samples=9, metric=metric)\n",
        "    #fit/transform with record dataset\n",
        "    dbscan_9_predict = dbscan.fit_predict(subset_combined)\n",
        "\n",
        "    #now we should have a 2 dimensional representation of original dataset\n",
        "    #plot the TSNE transformation with class labels for coloring\n",
        "    plt.subplot(2,2,count)\n",
        "    plt.scatter(subset_combined['vision_score/min'], subset_combined['minions/min'], c=dbscan_9_predict)\n",
        "    #labels\n",
        "    plt.title(f'DBSCAN with metric = {metric}')\n",
        "    plt.xlabel('Vision Score/Minute')\n",
        "    plt.ylabel('Minions/Minute')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "#save as .png\n",
        "plt.savefig('dbscan_cluster_metrics.png', bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![DBSCAN Clustering with Different Distance Metrics](images/dbscan_cluster_metrics.png)\n",
        "\n",
        "As mentioned before in the theory section of this tab, DBSCAN cluster had a particular aspect of figuring out outliers/noise. This is abundantly clear in my subset clustering where the outer points that were not really part of the dense areas of the distribution were considered part of a different cluster. I found this noise/outlier discovering pretty interesting and possibly useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results for Hierarchical Clustering\n",
        "\n",
        "Finally, I looked into Hierarchical Clustering. As mentioned before there were two main approaches: Agglomerative and Divisive Clustering. Since Divisive Clustering is not as popular, I decided on focusing on Agglomerative Clustering for this Hierarchical Clustering technique.  <br> <br>\n",
        "\n",
        "For this technique, there was also another way to optimize the clustering. The use of linkage strategies to determine which clusters merge with each other was an important parameter that should be explored. From the ```Scikit-Learn``` Hierarchy Clustering library, I used the dendrogram function to find which linkage strategy worked best with the subset dataset. This is what I found using dendrograms: <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#list of all linkage strats to compare dendrograms\n",
        "linkages = ['single','average','complete','ward']\n",
        "count = 0\n",
        "\n",
        "for strat in linkages:\n",
        "    count += 1 #helps make subplot\n",
        "    plt.subplot(2,2,count) #create 4 subplots\n",
        "\n",
        "    #create dendrogram to determine which linkage strat to use later\n",
        "    dendro_dist = linkage(subset_combined,  method=strat)\n",
        "    dendrogram(dendro_dist)\n",
        "    plt.title(f'Dendrogram with {strat} Linkage')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "#save .png\n",
        "plt.savefig('dendro_linkage.png', bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "![Dendrogram of Different Linkage Strategies](images/dendro_linkage.png)\n",
        "\n",
        "To determine which linkage strategies worked best with my dataset, I needed to examine the lengths of vertical lines for clusters and how balanced it was throughout the different merges. Additionally, it was important to see which strategies offered the most distinct clustering. From the plot, it seems that the \"Complete\" Linkage strategy had an overall balanced assortment of branch lengths and distinct clustering. The next one that had a something similar was the \"Average\" linkage strategy.<br> <br>\n",
        "\n",
        "After concluding the use of the \"Complete\" linkage strategy, I decided to fit and predict cluster labels of my original dataset to see how well the clustering occurred. <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#use n_clusters=None to not limit the number of clusters\n",
        "#distance_threshold=0 to not cut the dendrogram when forming clusters\n",
        "agg_complete = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='complete')\n",
        "\n",
        "subset_predict = agg_complete.fit_predict(subset_combined)\n",
        "\n",
        "\n",
        "plt.scatter(subset_combined['vision_score/min'], subset_combined['minions/min'], c=subset_predict)\n",
        "#labels\n",
        "plt.title('Agglomerative Clustering With Complete Linkage')\n",
        "plt.xlabel('Vision Score/Minute')\n",
        "plt.ylabel('Minions/Minute')\n",
        "\n",
        "#save .png\n",
        "plt.savefig('agg_complete_linkage.png', bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "![Dendrogram of Different Linkage Strategies](images/agg_complete_linkage.png)\n",
        "\n",
        "The use of the \"Complete\" linkage strategy seemed to show a pretty distinct clustering with 2 clusters being distinctly formed. This result was what I expected considering I used two features for a 2-dimensional dataset for all the clustering techniques. Since Agglomerative Clustering starts off as each data point being their own cluster, that describes why there are so many other colors but the two clusters being shown eventually describes how the merging occurred at two distinct areas of the distribution. This helps confirm the intuition that two clusters should form for the two unique features used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing All Three on my Project's Idea/Dataset\n",
        "\n",
        "Overall, I think the use of the three techinques brought about some interesting thoughts for my project dataset. First off, I realized that K-Means did not really show promising results for my lower-dimensional subset from my original dataset. I would only suspect the technique to work even worse with my actual dataset. Secondly, the use of DBSCAN as a clustering method really emphasized its use in detecting noise or outliers within the dataset. To be able to find the general cluster of data and then separate the more noisy looking data was very interesting to see. Finally, the use of dendrograms and Hierarchical Clustering to visually explore how interrelated data points and features were very validating to me. It helped me confirm my intuition that there should be two distinct clusters for my subset of data as I chose to plot two unique features together. The relationship of using an optimal linkage strategy helped in showing how similar data points were to each other. <br> <br>\n",
        "\n",
        "In the end, I felt that K-means was definitely the easiest technique to grasp but it did not really provide as much insight as the other two techniques with my, in my opinion, relatively complex dataset. In the end, I felt that I found supporting evidence that my feature selected variables might indeed be independent enough with the use of Hierarchical Clustering. Additionally, I realized that my dataset might include a decent amount of noise with the use of DBSCAN clustering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusions:\n",
        "\n",
        "As mentioned before, my dataset does seem to provide independent/unique features. To be able to use it in a classification model might prove to be difficult still because of my discovery of a decent amount of noise in feature selected data. I believe in the future if I were to work on this project more, I would look into more unique variables and consider techniques in removing noise or outliers that might skew my classification model. \n",
        "<br> <br>\n",
        "\n",
        "In regards to my overall scope of enhancing Skill Based Matchmaking Systems for competitive video games, I think this clustering sub-project has taught me that the emphasis of certain traits of a game could help in creating a more balanced matches between players but the impact of variables are seemingly blurry to make a definitive answer. I think the weights of certain traits of players could be adjusted in determining skill rating but that would require more research."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
